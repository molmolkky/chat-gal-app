# ui/evaluation_ui.py
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import pandas as pd

from backend.evaluation import evaluation_service

class EvaluationUI:
    def __init__(self):
        self.evaluation_service = evaluation_service

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆè¡¨ç¤ºå â†’ ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰åï¼‰
        self.metrics_mapping = {
            "Context Precision": "context_precision",
            "Context Recall": "context_recall", 
            "Faithfulness": "faithfulness",
            "Answer Relevancy": "answer_relevancy"
        }
        
        # é€†ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å â†’ è¡¨ç¤ºåï¼‰
        self.metrics_display_mapping = {v: k for k, v in self.metrics_mapping.items()}
    
    def render_evaluation_summary(self):
        """è©•ä¾¡ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        st.subheader("ğŸ“Š è©•ä¾¡ã®çµæœã ã‚ˆã€œ")
        
        summary = self.evaluation_service.get_evaluation_summary()
        
        if not summary:
            st.info("ã¾ã è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã‚ˆã€œã€‚ãƒãƒ£ãƒƒãƒˆã§è³ªå•ã—ã¦ã‹ã‚‰è©•ä¾¡ã—ã¦ã¿ã¦ğŸ’•")
            return
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("ç·ãƒãƒ£ãƒƒãƒˆæ•°", summary.get("total_chats", 0))
        
        with col2:
            st.metric("è©•ä¾¡æ¸ˆã¿", summary.get("evaluated_chats", 0))
        
        with col3:
            if "avg_overall_score" in summary:
                st.metric("å¹³å‡ç·åˆã‚¹ã‚³ã‚¢", f"{summary['avg_overall_score']:.3f}", help="å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã‚¹ã‚³ã‚¢ã®å¹³å‡ã§ã‚ã‚‹ã€Œç·åˆã‚¹ã‚³ã‚¢ã€ã®å¹³å‡ã‚’è¡¨ç¤ºã—ã¦ã‚‹ã‚ˆã€œ")
            else:
                st.metric("å¹³å‡ç·åˆã‚¹ã‚³ã‚¢", "æœªè©•ä¾¡", help="å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã‚¹ã‚³ã‚¢ã®å¹³å‡ã§ã‚ã‚‹ã€Œç·åˆã‚¹ã‚³ã‚¢ã€ã®å¹³å‡ã‚’è¡¨ç¤ºã—ã¦ã‚‹ã‚ˆã€œ")
        
        with col4:
            evaluation_rate = 0
            if summary.get("total_chats", 0) > 0:
                evaluation_rate = summary.get("evaluated_chats", 0) / summary["total_chats"] * 100
            st.metric("è©•ä¾¡ç‡", f"{evaluation_rate:.1f}%")
    
    def render_metrics_chart(self):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ãƒãƒ£ãƒ¼ãƒˆã‚’è¡¨ç¤º"""
        if "evaluation_data" not in st.session_state:
            return
        
        data = st.session_state.evaluation_data
        evaluated_data = [item for item in data if item.overall_score is not None]
        
        if not evaluated_data:
            return
        
        st.subheader("ğŸ“ˆ ã‚°ãƒ©ãƒ•ã§è¦‹ã¦ã¿ã‚ˆã€œ")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        metrics_data = []
        for item in evaluated_data:
            if item.context_precision is not None:
                metrics_data.append({"metric": "Context Precision", "score": item.context_precision, "timestamp": item.timestamp})
            if item.context_recall is not None:
                metrics_data.append({"metric": "Context Recall", "score": item.context_recall, "timestamp": item.timestamp})
            if item.faithfulness is not None:
                metrics_data.append({"metric": "Faithfulness", "score": item.faithfulness, "timestamp": item.timestamp})
            if item.answer_relevancy is not None:
                metrics_data.append({"metric": "Answer Relevancy", "score": item.answer_relevancy, "timestamp": item.timestamp})
        
        if metrics_data:
            df = pd.DataFrame(metrics_data)
            
            # ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ
            fig = px.box(df, x="metric", y="score", title="ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®åˆ†å¸ƒğŸ’")
            st.plotly_chart(fig, use_container_width=True)
            
            # æ™‚ç³»åˆ—ãƒãƒ£ãƒ¼ãƒˆ
            fig2 = px.line(df, x="timestamp", y="score", color="metric", title="æ™‚ç³»åˆ—ã§è¦‹ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å¤‰åŒ–âœ¨")
            st.plotly_chart(fig2, use_container_width=True)
    
    def render_evaluation_controls(self):
        """è©•ä¾¡åˆ¶å¾¡UI"""
        st.subheader("ğŸ”§ è©•ä¾¡ã®è¨­å®š")
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹é¸æŠ
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®èª¬æ˜
            metrics_help = {
                "Context Precision": "æ¤œç´¢ã§æ‹¾ã£ãŸæ–‡è„ˆã€è³ªå•ã¨ãƒãƒãƒãƒåˆã£ã¦ã‚‹ç‡ã ã‚ˆğŸ’… ä½™è¨ˆãªã®ã¯å³ã‚ªãƒ•ã€œ",
                "Context Recall": "ç­”ãˆã«å¿…è¦ãªãƒã‚¿ã€æ–‡è„ˆã«ã©ã‚Œã ã‘ç››ã‚Œã¦ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã‚„ã¤âœ¨ å–ã‚Šã“ã¼ã—ã‚¼ãƒ­ç›®æŒ‡ãã€œ",
                "Faithfulness": "ç”Ÿæˆã—ãŸç­”ãˆã€æ–‡è„ˆã«ã‚¬ãƒå¿ å®Ÿã‹è¦‹ã‚‹ã‚„ã¤ğŸ“š å¦„æƒ³æ··ãœãŸã‚‰ä¸€ç™ºã‚¢ã‚¦ãƒˆã ã‚ˆğŸ‘€",
                "Answer Relevancy": "ç­”ãˆã®å†…å®¹ã€è³ªå•ã¨ç¥ä¸€è‡´ã—ã¦ã‚‹ã‹ã®æ¸©åº¦æ„Ÿã ã‚ˆğŸ”¥ ãšã‚Œã¦ãŸã‚‰ã€ãã‚Œã¯ãã‚Œã€ã˜ã‚ƒãªãã¦NGã€œ"
            }

            # è¡¨ç¤ºåã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’é¸æŠ
            selected_display_metrics = st.multiselect(
                "è©•ä¾¡ã—ãŸã„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’é¸ã‚“ã§ã­ã€œ",
                options=list(self.metrics_mapping.keys()),
                default=["Context Precision", "Context Recall"],
                help="ã©ã®é …ç›®ã§è©•ä¾¡ã™ã‚‹ã‹é¸ã‚“ã§ã­ğŸ’•"
            )

            # é¸æŠã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®èª¬æ˜ã‚’è¡¨ç¤º
            if selected_display_metrics:
                st.write("**é¸ã‚“ã ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®èª¬æ˜:**")
                for metric in selected_display_metrics:
                    st.write(f"â€¢ **{metric}**: {metrics_help[metric]}")

            # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ç”¨ã«å¤‰æ›
            selected_metrics = [self.metrics_mapping[metric] for metric in selected_display_metrics]
        
        with col2:
            st.write("") # ã‚¹ãƒšãƒ¼ã‚¹
            st.write("") # ã‚¹ãƒšãƒ¼ã‚¹
            if st.button("ğŸš€ è©•ä¾¡ã‚¹ã‚¿ãƒ¼ãƒˆï¼", type="primary", disabled=not selected_metrics):
                self.run_evaluation(selected_metrics)
    
    def run_evaluation(self, selected_metrics):
        """è©•ä¾¡ã‚’å®Ÿè¡Œ"""
        if "evaluation_data" not in st.session_state or not st.session_state.evaluation_data:
            st.warning("è©•ä¾¡ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã‚ˆã€œ")
            return
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        def progress_callback(message):
            status_text.text(message)
        
        try:
            # è©•ä¾¡å®Ÿè¡Œ
            evaluated_data = self.evaluation_service.evaluate_all_chats(
                selected_metrics, 
                progress_callback
            )
            
            progress_bar.progress(1.0)
            status_text.text("âœ… è©•ä¾¡å®Œäº†ã€œï¼")
            
            st.success(f"ğŸ‰ {len(evaluated_data)}ä»¶ã®ãƒãƒ£ãƒƒãƒˆã‚’è©•ä¾¡ã—ãŸã‚ˆã€œï¼")
            st.rerun()
            
        except Exception as e:
            st.error(f"âŒ è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¡ã‚ƒã£ãŸğŸ’¦: {str(e)}")
            status_text.text("ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
    
    def render_evaluation_details(self):
        """è©•ä¾¡è©³ç´°ã‚’è¡¨ç¤º"""
        if "evaluation_data" not in st.session_state:
            return
        
        st.subheader("ğŸ“‹ è©³ã—ã„è©•ä¾¡çµæœ")
        
        data = st.session_state.evaluation_data
        
        if not data:
            st.info("è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã‚ˆã€œ")
            return
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        col1, col2 = st.columns([1, 1])
        
        with col1:
            show_evaluated_only = st.checkbox("è©•ä¾¡æ¸ˆã¿ã ã‘è¦‹ã‚‹", value=False)
        
        with col2:
            sort_by = st.selectbox("ä¸¦ã³é †", ["æ™‚é–“é †", "ã‚¹ã‚³ã‚¢é †"], index=0)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ»ã‚½ãƒ¼ãƒˆ
        filtered_data = data
        if show_evaluated_only:
            filtered_data = [item for item in data if item.overall_score is not None]
        
        if sort_by == "ã‚¹ã‚³ã‚¢é †" and filtered_data:
            filtered_data = sorted(filtered_data, key=lambda x: x.overall_score or 0, reverse=True)
        
        # ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
        for i, item in enumerate(filtered_data):
            with st.expander(f"ğŸ’¬ {item.question[:50]}... ({item.timestamp.strftime('%Y-%m-%d %H:%M')})"):
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    st.write("**è³ªå•:**")
                    st.write(item.question)
                    st.write("**å›ç­”:**")
                    st.write(item.answer)
                    st.write("**å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«:**")
                    st.write(", ".join(item.source_files))
                
                with col2:
                    if item.overall_score is not None:
                        st.metric("ç·åˆã‚¹ã‚³ã‚¢", f"{item.overall_score:.3f}", help="å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã‚¹ã‚³ã‚¢ã®å¹³å‡ã ã‚ˆã€œ")
                        
                        if item.context_precision is not None:
                            st.metric("Context Precision", f"{item.context_precision:.3f}")
                        if item.context_recall is not None:
                            st.metric("Context Recall", f"{item.context_recall:.3f}")
                        if item.faithfulness is not None:
                            st.metric("Faithfulness", f"{item.faithfulness:.3f}")
                        if item.answer_relevancy is not None:
                            st.metric("Answer Relevancy", f"{item.answer_relevancy:.3f}")
                    else:
                        st.info("ã¾ã è©•ä¾¡ã—ã¦ãªã„ã‚ˆã€œ")
    
    def render_data_management(self):
        """ãƒ‡ãƒ¼ã‚¿ç®¡ç†UI"""
        st.subheader("ğŸ—‚ï¸ ãƒ‡ãƒ¼ã‚¿ã®ç®¡ç†")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸ“¥ CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"):
                df = self.evaluation_service.export_evaluation_data()
                if not df.empty:
                    csv = df.to_csv(index=False)
                    st.download_button(
                        label="ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹",
                        data=csv,
                        file_name=f"evaluation_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
                else:
                    st.warning("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã‚ˆã€œ")
        
        with col2:
            if st.button("ğŸ—‘ï¸ ãƒ‡ãƒ¼ã‚¿å…¨å‰Šé™¤", type="secondary"):
                if st.checkbox("æœ¬å½“ã«å…¨éƒ¨æ¶ˆã—ã¡ã‚ƒã†ï¼Ÿ"):
                    self.evaluation_service.clear_evaluation_data()
                    st.success("ãƒ‡ãƒ¼ã‚¿ã‚’å…¨éƒ¨æ¶ˆã—ãŸã‚ˆã€œ")
                    st.rerun()
    
    def render_evaluation_page(self):
        """è©•ä¾¡ãƒšãƒ¼ã‚¸å…¨ä½“ã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°"""
        st.header("RAGã®æ€§èƒ½ãƒã‚§ãƒƒã‚¯âœ¨")
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        self.render_evaluation_summary()
        
        st.divider()
        
        # è©•ä¾¡åˆ¶å¾¡
        self.render_evaluation_controls()
        
        st.divider()
        
        # ãƒãƒ£ãƒ¼ãƒˆè¡¨ç¤º
        self.render_metrics_chart()
        
        st.divider()
        
        # è©³ç´°è¡¨ç¤º
        self.render_evaluation_details()
        
        st.divider()
        
        # ãƒ‡ãƒ¼ã‚¿ç®¡ç†
        self.render_data_management()

def render_evaluation_page():
    """è©•ä¾¡ãƒšãƒ¼ã‚¸ã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°"""
    evaluation_ui = EvaluationUI()
    evaluation_ui.render_evaluation_page()